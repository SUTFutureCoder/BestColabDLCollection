{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUTFutureCoder/BestColabDLCollection/blob/master/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRNYHWwYyd4Q",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Neural Network\n",
        "\n",
        "**Authors:** Yen-Ling Kuo and Eugenio Piasini for the [Brains, Minds and Machines summer course 2018](http://cbmm.mit.edu/summer-school/2018)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi_zEx8qlmM-",
        "colab_type": "text"
      },
      "source": [
        "At this point, we have seen how to implement various feed-forward networks. In this tutorial, we will learn how to build recurrent networks that maintain the states while processing sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5L-MMTkybU5",
        "colab_type": "code",
        "outputId": "90c64428-a9ab-44c5-d134-70c4be721e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip3 install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iCUx8oylaT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bJvq3okegMw",
        "colab_type": "text"
      },
      "source": [
        "### Simple RNN to Learn Sine Wave\n",
        "\n",
        "First, we are going to build a simple RNN that can reproduce the sine wave we provide for training.\n",
        "\n",
        "The input tensor to a RNN has size (sequence_length, batch_size, input_size). We will need to turn our input data to the same size using [view(.)](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
        "\n",
        "*   `sequence_length` is the length of the sine wave. Here, we put -1 to match any length if other dimensions have fixed size.\n",
        "*   `batch_size` is the number of sequences we are going to train together. Since we only have one input sine wave, we have an 1 here.\n",
        "*  `input_size` is the number of features at each time step. For sine wave, we have one x value a time, thus, an 1 here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILfN_iACmFpD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "7deb3863-d54f-48eb-f661-e45984cf4102"
      },
      "source": [
        "num_time_steps = 10\n",
        "\n",
        "time_steps = np.linspace(0, 2*np.pi, num_time_steps)\n",
        "data = np.sin(time_steps)\n",
        "xs = data[:-1]\n",
        "ys = data[1:]\n",
        "print(data[:-1])\n",
        "\n",
        "train_x = torch.Tensor(xs).view(-1, 1, 1)\n",
        "train_y = torch.Tensor(ys)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.          0.64278761  0.98480775  0.8660254   0.34202014 -0.34202014\n",
            " -0.8660254  -0.98480775 -0.64278761]\n",
            "tensor([[[ 0.0000]],\n",
            "\n",
            "        [[ 0.6428]],\n",
            "\n",
            "        [[ 0.9848]],\n",
            "\n",
            "        [[ 0.8660]],\n",
            "\n",
            "        [[ 0.3420]],\n",
            "\n",
            "        [[-0.3420]],\n",
            "\n",
            "        [[-0.8660]],\n",
            "\n",
            "        [[-0.9848]],\n",
            "\n",
            "        [[-0.6428]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAgKu8YfUOCf",
        "colab_type": "text"
      },
      "source": [
        "The simplest RNN we can build consists of a RNN cell and a fully connected layer to readout the output value from the hidden states.\n",
        "\n",
        "In the forward method, we unroll the the recurrent computation for each time step in the for loop.\n",
        "\n",
        "In addition to the predicted output values, the forward method also returns a vector for the hidden state so we can continue building a sequence based on the hidden state from previous time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuhDahgPeQ5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=1,\n",
        "                          hidden_size=hidden_size,\n",
        "                          num_layers=1)\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        " \n",
        "    def forward(self, seq, hc=None):\n",
        "        tmp, hc = self.rnn(seq, hc)\n",
        "        out = self.linear(tmp)\n",
        "        return out, hc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSyBx8X4FJpx",
        "colab_type": "text"
      },
      "source": [
        "We can set the hyperparameters and run training as before. Again, we use mean squared error to compute the loss for the real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AisNWPe8m0yC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 16\n",
        "learning_rate = 0.01\n",
        "\n",
        "model = SimpleRNN(hidden_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96UrZAK5mrD5",
        "colab_type": "code",
        "outputId": "7294e546-6dfb-442e-bb19-24b8f648629c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output, _ = model(train_x)\n",
        "    loss = criterion(output.view(-1), train_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch {}: loss {}\".format(epoch, loss.item()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: loss 0.41650545597076416\n",
            "Epoch 10: loss 0.007402567192912102\n",
            "Epoch 20: loss 0.01967959851026535\n",
            "Epoch 30: loss 0.0025746673345565796\n",
            "Epoch 40: loss 0.002394310664385557\n",
            "Epoch 50: loss 0.0005844995030201972\n",
            "Epoch 60: loss 0.00037750767660327256\n",
            "Epoch 70: loss 8.044004061957821e-05\n",
            "Epoch 80: loss 5.819283614982851e-05\n",
            "Epoch 90: loss 3.746922448044643e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCxoY4nqGTQz",
        "colab_type": "text"
      },
      "source": [
        "To re-generate the sine wave, we need to provide the x values to the network. A tensor for the hidden state is maintained over the course of the prediction. Initially, it is None, which means the RNN will initialize the vector to all zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Gi99gGnCCc",
        "colab_type": "code",
        "outputId": "ec5c30db-8c97-48b7-9a31-dc30b0aa35e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "predictions = []\n",
        "input = train_x[0, :, :]  # choose the first time step as input\n",
        "hidden_prev = None\n",
        "for i in range(train_x.shape[0]):\n",
        "    input = train_x[i, :, :]\n",
        "    print(train_x[i])\n",
        "    input = input.view(1, 1, 1)\n",
        "    pred, hidden_prev = model(input, hidden_prev)\n",
        "    predictions.append(pred.data.numpy()[0])\n",
        "\n",
        "x = train_x.data.numpy()\n",
        "fig, ax = plt.subplots()\n",
        "plt.scatter(time_steps[:-1], x, s=90, label='actual')\n",
        "plt.scatter(time_steps[1:], predictions, label='predicted')\n",
        "ax.legend()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.]])\n",
            "tensor([[0.6428]])\n",
            "tensor([[0.9848]])\n",
            "tensor([[0.8660]])\n",
            "tensor([[0.3420]])\n",
            "tensor([[-0.3420]])\n",
            "tensor([[-0.8660]])\n",
            "tensor([[-0.9848]])\n",
            "tensor([[-0.6428]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f789a9a8668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gasqgsH2I2Tg",
        "colab_type": "text"
      },
      "source": [
        "The predicted values seem pretty well-aligned with the training sequence. You can try to give it a different input value to see how robust it is to different initial values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngD8aB-7f8am",
        "colab_type": "text"
      },
      "source": [
        "### LSTM for POS Tagging\n",
        "\n",
        "In [the lecture](https://drive.google.com/open?id=1NG9wnv3V80PAayuXn4FYQtC5Ss0cpggUr8nES3a1e_s), we showed the example to get the part of speech tag for each word in a sentence. In this section, we will implement an LSTM-based tagger for this task!\n",
        "\n",
        "The input data are sentences and their POS tags. We need to map the words to indexes so it is easier to manipulate in tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39IB923EgFlP",
        "colab_type": "code",
        "outputId": "fb0e133c-4222-4e80-bf36-2a7df1ec0739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "\n",
        "word_to_ix = {}\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
        "print(tag_to_ix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
            "{'DET': 0, 'NN': 1, 'V': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVlb8sriMykt",
        "colab_type": "text"
      },
      "source": [
        "The model first represent words using word embeddings. Word embeddings are dense vectors of real numbers, one per word in your vocabulary. [`nn.Embedding`](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding) allows us to build a lookup table to store the embeddings. It is also possible to use pre-trained word embeddings in your model (read example [here](https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76)).\n",
        "\n",
        "Different from the RNNs, LSTMs also mantain a cell state. So, we can find the input and output tuple that contains a tensor for the hidden state and another tensor for the cell state. Both are initialized to all zeros. They have size (num_layers, batch_size, hidden_dim).\n",
        "\n",
        "*  `num_layers`: We can have multiple LSTMs stacked on top of the others. If not specified, the default is one LSTM.\n",
        "*  `batch_size`: This is the number of sentences we are going to train together. In this example, we train with one sentence a time.\n",
        "*  `hidden_dim`: This is the length of vectors for the states. \n",
        "\n",
        "Similar to the example in the CNN tutorial, we take softmax of the affine map of the hidden state to output the distribution over the label classes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRNBF76rgsWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_dim),\n",
        "                torch.zeros(1, 1, self.hidden_dim))\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahAz093rpaY5",
        "colab_type": "text"
      },
      "source": [
        "We have a function to convert a word to an index so that we can feed it to the input. As before, we use negative log likelihood as loss function.\n",
        "\n",
        "The dimensions for word embedding and hidden states are quite small. Usually, you need a much larger size for real dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEcFn9oAhKPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "  \n",
        "embedding_dim = 6\n",
        "hidden_dim = 6\n",
        "\n",
        "model = LSTMTagger(embedding_dim, hidden_dim, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKgA8W9qtzbV",
        "colab_type": "text"
      },
      "source": [
        "Following the same steps, we train the LSTM model and check out the predicted tag for each word!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kdebc5Xhhj7",
        "colab_type": "code",
        "outputId": "ff504833-a7af-42b6-8405-2a921b0d1fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "for epoch in range(300):\n",
        "    for sentence, tags in training_data:\n",
        "        optimizer.zero_grad()\n",
        "        model.hidden = model.init_hidden()\n",
        "\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 50 == 0:\n",
        "        print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
        "\n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    _, predicted = torch.max(tag_scores, 1)\n",
        "    print(training_data[0][0])\n",
        "    print(predicted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, loss 1.0691885948181152\n",
            "epoch 50, loss 0.8439609408378601\n",
            "epoch 100, loss 0.46415096521377563\n",
            "epoch 150, loss 0.21308235824108124\n",
            "epoch 200, loss 0.10027572512626648\n",
            "epoch 250, loss 0.058674246072769165\n",
            "['The', 'dog', 'ate', 'the', 'apple']\n",
            "tensor([0, 1, 2, 0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD3XyQWrerRD",
        "colab_type": "text"
      },
      "source": [
        "### Exercise: Generating Shakespeare\n",
        "\n",
        "In the example above, we work on the word level classification. However, there are cases we may want to generate character one at a time. For example, deciding which character we want to capitalize or generating reasonable names, etc.\n",
        "\n",
        "In this exercise, you will need to implement a model similar to what we have before but at character level to generate Shakespeare style texts.\n",
        "\n",
        "First, let's download the training data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsnaBYfVn_bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_file = 'input.txt'\n",
        "!if [ ! -f $input_file ]; then wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt; fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4R0XTjYyep7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Instead of input a big file for training, we will pick random chunks from the file as the input training sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqVzUPdclIfA",
        "colab_type": "code",
        "outputId": "3ec4762c-617f-43b1-a2a7-e9b4dc2abdfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip3 install unidecode\n",
        "\n",
        "import random\n",
        "import unidecode\n",
        "\n",
        "file = unidecode.unidecode(open('input.txt').read())\n",
        "\n",
        "def random_chunk(chunk_len=200):\n",
        "    start_index = random.randint(0, len(file) - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.0.22)\r\n",
            "ps of citizens to speak with him,\n",
            "His grace not being warn'd thereof before:\n",
            "My lord, he fears you mean no good to him.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Sorry I am my noble cousin should\n",
            "Suspect me, that I mean no good to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f2y7UkYyzYr",
        "colab_type": "text"
      },
      "source": [
        "Similar to words, we convert words to integer indexes. Each character now has a unique id.\n",
        "\n",
        "We also provide a function to produce a random training sequence. The input is the character at time $t$. The target is the expected character to see at $t+1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3ZlkOdeoezN",
        "colab_type": "code",
        "outputId": "480a6219-67c7-4338-cdc5-cd89952be900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import string\n",
        "\n",
        "# Turn string into list of longs\n",
        "def char_tensor(str):\n",
        "    tensor = torch.zeros(len(str), 1).long()\n",
        "    for c in range(len(str)):\n",
        "        tensor[c][0] = string.printable.index(str[c])\n",
        "    return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))\n",
        "\n",
        "def random_train_data():    \n",
        "    chunk = random_chunk()\n",
        "    input = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return input, target"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[10],\n",
            "        [11],\n",
            "        [12],\n",
            "        [39],\n",
            "        [40],\n",
            "        [41]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8akCRqJ0hKD",
        "colab_type": "text"
      },
      "source": [
        "Now, it's your turn to implement the model!\n",
        "\n",
        "This model will take as input the character for time step $t$ and is expected to output the character at $t+1$. There are three layers - one embedding layer that encodes the input character into a dense vector, one GRU layer (yes, let's try GRU instead of LSTM this time!) that operates on that dense vector and a hidden state, and a decoder layer that outputs the probability distribution over all possible characters.\n",
        "\n",
        "You can take a look at the GRU documentation ([`nn.GRU`](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU)), the usage is very similar to RNNs and LSTMs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPdaJ9l5pENa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShakespeareRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(ShakespeareRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        # TODO: design your structure\n",
        "\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        # TODO: complete your forward pass\n",
        "\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N70ZNCM1v0F",
        "colab_type": "text"
      },
      "source": [
        "To check out the generation result, we provide you an evaluate function so you can pick any character to start with to generate a paragraph using the trained model.\n",
        "\n",
        "The generation process works as follows. We will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlyew-WXple9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, prime_str='A', predict_len=100):\n",
        "    hidden = model.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = model(prime_input[p], hidden)\n",
        "    input = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = model(input, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = string.printable[top_i]\n",
        "        predicted += predicted_char\n",
        "        input = char_tensor(predicted_char)\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ism4bEwA2fPF",
        "colab_type": "text"
      },
      "source": [
        "Finally, we define the training parameters, instantiate the model, and start training. \n",
        "\n",
        "Please complete the missing training script! Once you start training, you can see how the generated texts evolve every 100 epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPVERl79rCYW",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "12bafad1-2876-4f09-84b0-2476267fe379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2346
        }
      },
      "source": [
        "n_characters = len(string.printable)\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "\n",
        "epochs = 2000\n",
        "lr = 0.005\n",
        "\n",
        "model = ShakespeareRNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss = 0\n",
        "    input, target = random_train_data()\n",
        "    chunk_len = len(input)\n",
        "    \n",
        "    # TODO: complete the training script\n",
        "\n",
        "\n",
        "    loss = loss.item() / chunk_len\n",
        "    if epoch % 100 == 0:\n",
        "        print('[(%d %d%%) %.4f]' % (epoch, epoch / epochs * 100, loss))\n",
        "        print(evaluate(model, 'Wh', 100), '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0 0%) 4.6329]\n",
            "WhY)`E\t%O+fE(7c^@uj=H\\xl>\rz\n",
            "Mh`!?^&g;!xX2\u000b\t?H^oU0z$3\fyk$8K7D<[m 6y8zff#8n~QPRXy $QT`+;\t&7yA_MTo7].v:+# \n",
            "\n",
            "[(100 5%) 2.3615]\n",
            "Whak! maft; weeku\n",
            "An's girsg\n",
            "Ho wo I I or\n",
            "Silay.\n",
            "LARGpeante\n",
            " of.\n",
            "\n",
            "As:\n",
            "Whour derou, lave to thiI sit co \n",
            "\n",
            "[(200 10%) 2.2515]\n",
            "Whtoble mach Banke; Wesoulle:\n",
            "The coothe un it.\n",
            "\n",
            "PRERme his hast as by Eeind rour, lows yas\n",
            "\n",
            "Cllllist\n",
            " \n",
            "\n",
            "[(300 15%) 2.2235]\n",
            "Wh'd paich a dughseirseel deere that;\n",
            "Chave plizeld my wiche forne| hear;\n",
            "Mut an ame gondagseare; his! \n",
            "\n",
            "[(400 20%) 2.0030]\n",
            "Whpanting let the loved.\n",
            "'Tid would a what grangengs;, will the wrerede ford vill not wold,\n",
            "Misn linef \n",
            "\n",
            "[(500 25%) 1.8342]\n",
            "Whe hik will wire I will Kik.\n",
            "\n",
            "SAYFBY BARD:\n",
            "Celver my ginligan of the that thee's ner.\n",
            "\n",
            "RIORNUCES:\n",
            "I r \n",
            "\n",
            "[(600 30%) 1.8704]\n",
            "Whough and likgher mant.\n",
            "\n",
            "SSA:\n",
            "Ding seesp ling of a dots of tell; this go himbye,\n",
            "Theres:\n",
            "The kissh Mu \n",
            "\n",
            "[(700 35%) 1.9027]\n",
            "Whe, you welp gomgh, his, that: I baily,\n",
            "Mer thaup'ld bonous may know ank weuld surfo?\n",
            "\n",
            "DUCIUS:\n",
            "Ma so  \n",
            "\n",
            "[(800 40%) 2.0641]\n",
            "Whe carster my goost nond, notion!\n",
            "Noo, and condery thine that be in dignare,\n",
            "The thee a no,\n",
            "Than emar \n",
            "\n",
            "[(900 45%) 1.9868]\n",
            "What bloodran.\n",
            "And, bid you lose therghan the tit me the remopor'd peat;\n",
            "At hopers my you bato,\n",
            "Lervie \n",
            "\n",
            "[(1000 50%) 2.0915]\n",
            "Whan suriought guards, and I'll please Rold not sorrow.\n",
            "Of vits died Se ough stimes leave meate;\n",
            "Adme  \n",
            "\n",
            "[(1100 55%) 1.9849]\n",
            "Whou, York toose\n",
            "Which is fathars your congair's cirt-tupssow couldd,\n",
            "To nate, I well wan'd dost, as m \n",
            "\n",
            "[(1200 60%) 2.0570]\n",
            "Whes: affole cheep it connitch.\n",
            "\n",
            "ASCERTIUY\n",
            "OLI:\n",
            "Sidin age, come, and abcoma dit sleech heir?\n",
            "So dawife \n",
            "\n",
            "[(1300 65%) 1.9482]\n",
            "Whisher Ertfoled os\n",
            "Maty pore. for of Gok not in to day sgnoth,\n",
            "And lord'ss your given of. Service ela \n",
            "\n",
            "[(1400 70%) 1.7126]\n",
            "Whad, tose I him? life.\n",
            "We I orrion, men a, shat his falieves\n",
            "ROS:\n",
            "cut ares, you praming all beor,\n",
            "Tha \n",
            "\n",
            "[(1500 75%) 2.1257]\n",
            "Wh:\n",
            "Nour four hor beture boness. Lever.\n",
            "\n",
            "ROMEO:\n",
            "Henauch I'll marrulked to, struke he singele's, therse \n",
            "\n",
            "[(1600 80%) 1.6570]\n",
            "Whou not EDWARS:\n",
            "Dor I, this bed, I and yours and then Is't\n",
            "I indery, mage? I ace, it as my his cay so \n",
            "\n",
            "[(1700 85%) 1.9039]\n",
            "Why, haster.\n",
            "\n",
            "iMS MARIUS:\n",
            "Tere chate mardy not broy.\n",
            "\n",
            "GLOUCESTIO:\n",
            "to me Had lord, spelat full disto;\n",
            "B \n",
            "\n",
            "[(1800 90%) 1.9156]\n",
            "What to bee\n",
            "Thay dish in borterous are Richrie the licked forgh'd time cily?\n",
            "\n",
            "HENRY Paust of came ofse \n",
            "\n",
            "[(1900 95%) 1.9599]\n",
            "What defity!\n",
            "Lintting theref egathinn.\n",
            "To uncan we own by fromed,\n",
            "Young thy ne!\n",
            "\n",
            "KING RICHARD II:\n",
            "Wemi \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMEsgK0s9Mr7",
        "colab_type": "text"
      },
      "source": [
        "### Congrats! \n",
        "\n",
        "You have finished the whole tutorial series.\n",
        "\n",
        "Feel free to explore different models with the tools you just learned and let us know if you have any question!"
      ]
    }
  ]
}